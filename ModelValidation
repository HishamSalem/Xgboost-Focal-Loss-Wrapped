
def calculate_auc_per_bin(data, feature_cols, target_col, predicted_probs_col, metric='auc_roc'):
    auc_per_bin = []
    
    for feature in feature_cols:
        bins = data[feature].unique()
        for bin_value in bins:
            bin_mask = data[feature] == bin_value
            y_true = data.loc[bin_mask, target_col]
            y_scores = data.loc[bin_mask, predicted_probs_col]
            if metric == 'auc_roc':
                auc_value = roc_auc_score(y_true, y_scores)
            elif metric == 'auc_pr':
                auc_value = calculate_auc_pr(y_true, y_scores)
            else:
                raise ValueError("Unsupported metric. Use 'auc_roc' or 'auc_pr'.")
            auc_per_bin.append({
                'feature': feature,
                'bin': bin_value,
                'auc': auc_value
            })
    
    auc_df = pd.DataFrame(auc_per_bin)
    return auc_df

def calculate_feature_statistics(auc_df):
    feature_stats = auc_df.groupby('feature')['auc'].agg(['min', 'max', 'mean', 'std']).reset_index()
    return feature_stats

def evaluate_model(data, feature_cols, target_col, predicted_probs_col, metric='auc_roc'):
    auc_df = calculate_auc_per_bin(data, feature_cols, target_col, predicted_probs_col, metric)
    feature_stats = calculate_feature_statistics(auc_df)
    return auc_df, feature_stats
